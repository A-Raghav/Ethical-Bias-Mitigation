{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "`load_boston` has been removed from scikit-learn since version 1.2.\n",
      "\n",
      "The Boston housing prices dataset has an ethical problem: as\n",
      "investigated in [1], the authors of this dataset engineered a\n",
      "non-invertible variable \"B\" assuming that racial self-segregation had a\n",
      "positive impact on house prices [2]. Furthermore the goal of the\n",
      "research that led to the creation of this dataset was to study the\n",
      "impact of air quality but it did not give adequate demonstration of the\n",
      "validity of this assumption.\n",
      "\n",
      "The scikit-learn maintainers therefore strongly discourage the use of\n",
      "this dataset unless the purpose of the code is to study and educate\n",
      "about ethical issues in data science and machine learning.\n",
      "\n",
      "In this special case, you can fetch the dataset from the original\n",
      "source::\n",
      "\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "    target = raw_df.values[1::2, 2]\n",
      "\n",
      "Alternative datasets include the California housing dataset and the\n",
      "Ames housing dataset. You can load the datasets as follows::\n",
      "\n",
      "    from sklearn.datasets import fetch_california_housing\n",
      "    housing = fetch_california_housing()\n",
      "\n",
      "for the California housing dataset and::\n",
      "\n",
      "    from sklearn.datasets import fetch_openml\n",
      "    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "for the Ames housing dataset.\n",
      "\n",
      "[1] M Carlisle.\n",
      "\"Racist data destruction?\"\n",
      "<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n",
      "\n",
      "[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n",
      "\"Hedonic housing prices and the demand for clean air.\"\n",
      "Journal of environmental economics and management 5.1 (1978): 81-102.\n",
      "<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
      ": LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\I2044\\Anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "X_raw = data.data\n",
    "y_raw = data.target\n",
    "X = pd.get_dummies(X_raw)\n",
    "y = 1 * (y_raw == \">50K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute_name = f\"sex_Male\"\n",
    "\n",
    "privileged_groups = [{protected_attribute_name: 1}]\n",
    "unprivileged_groups = [{protected_attribute_name: 0}]\n",
    "\n",
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Statistical parity difference\"\n",
    "\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "        \n",
    "#random seed for calibrated equal odds prediction\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_standard_dataset(\n",
    "        df: pd.DataFrame,\n",
    "        protected_attribute_name: str,\n",
    "        label_name=\"class\",\n",
    "    ) -> StandardDataset:\n",
    "        return StandardDataset(\n",
    "            df,\n",
    "            label_name=label_name,\n",
    "            favorable_classes=[1],\n",
    "            protected_attribute_names=[protected_attribute_name],\n",
    "            privileged_classes=[[1]],\n",
    "        )\n",
    "# Step 1: Create the StandardDataset object\n",
    "dataset = _create_standard_dataset(pd.concat([X, y], axis=1), protected_attribute_name)\n",
    "\n",
    "# Step 2: train-test-val :: 15:3:3\n",
    "dataset_tv, dataset_test = dataset.split([0.7], shuffle=True, seed=42)\n",
    "dataset_train, dataset_valid = dataset_tv.split([0.7], shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER = StandardScaler()\n",
    "X_train = SCALER.fit_transform(dataset_train.features)\n",
    "y_train = dataset_train.labels.ravel()\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "thresh = 0.5 \n",
    "pos_ind = np.where(LR.classes_ == dataset_train.favorable_label)[0][0]\n",
    "\n",
    "dataset_valid_pred = dataset_valid.copy(deepcopy=True)\n",
    "X_valid = SCALER.fit_transform(dataset_valid_pred.features)\n",
    "dataset_valid_pred.scores = LR.predict_proba(X_valid)[:,pos_ind].reshape(-1,1)\n",
    "dataset_valid_pred.labels = dataset_valid_pred.scores > thresh\n",
    "\n",
    "ROC = RejectOptionClassification(\n",
    "    unprivileged_groups=unprivileged_groups, \n",
    "    privileged_groups=privileged_groups, \n",
    "    low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "    num_class_thresh=100, num_ROC_margin=50,\n",
    "    metric_name=metric_name,\n",
    "    metric_ub=metric_ub, metric_lb=metric_lb\n",
    ")\n",
    "ROC = ROC.fit(dataset_valid, dataset_valid_pred)\n",
    "\n",
    "# user sends test data to their ML model\n",
    "# the ML model makes predictions and computes prob\n",
    "# The user then needs to send prob scores (biased predictions) + their test data to ROC_mitigator()\n",
    "# we get unbiased predictions in return\n",
    "dataset_test_pred = dataset_test.copy(deepcopy=True)\n",
    "X_test = SCALER.fit_transform(dataset_test_pred.features)\n",
    "y_test = dataset_test_pred.labels\n",
    "dataset_test_pred.scores = LR.predict_proba(X_test)[:,pos_ind].reshape(-1,1)\n",
    "dataset_test_pred.labels = dataset_test_pred.scores > thresh\n",
    "\n",
    "\n",
    "\n",
    "# Metrics for the transformed test set\n",
    "dataset_transf_test_pred = ROC.predict(dataset_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2665362044165289\n",
      "0.8774242659322757\n"
     ]
    }
   ],
   "source": [
    "metric_unmit = ClassificationMetric(\n",
    "    dataset_test,\n",
    "    dataset_test_pred,\n",
    "    privileged_groups=privileged_groups,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    ")\n",
    "metric_mit = ClassificationMetric(\n",
    "    dataset_test,\n",
    "    dataset_transf_test_pred,\n",
    "    privileged_groups=privileged_groups,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    ")\n",
    "print(metric_unmit.disparate_impact())\n",
    "print(metric_mit.disparate_impact())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2665362044165289"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_unmit = ClassificationMetric(\n",
    "    dataset_test,\n",
    "    dataset_test_pred,\n",
    "    privileged_groups=privileged_groups,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    ")\n",
    "metric_unmit.disparate_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8641017497800425"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_mit = ClassificationMetric(\n",
    "    dataset_test,\n",
    "    dataset_transf_test_pred,\n",
    "    privileged_groups=privileged_groups,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    ")\n",
    "metric_mit.disparate_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
