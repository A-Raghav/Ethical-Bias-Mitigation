{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adverserial Debiasing (In-processing)\n",
    "Adversarial debiasing is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary’s ability to determine the protected attribute from the predictions. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit.\n",
    "\n",
    "**References**\n",
    "* B. H. Zhang, B. Lemoine, and M. Mitchell, “Mitigating Unwanted Biases with Adversarial Learning,” AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "`load_boston` has been removed from scikit-learn since version 1.2.\n",
      "\n",
      "The Boston housing prices dataset has an ethical problem: as\n",
      "investigated in [1], the authors of this dataset engineered a\n",
      "non-invertible variable \"B\" assuming that racial self-segregation had a\n",
      "positive impact on house prices [2]. Furthermore the goal of the\n",
      "research that led to the creation of this dataset was to study the\n",
      "impact of air quality but it did not give adequate demonstration of the\n",
      "validity of this assumption.\n",
      "\n",
      "The scikit-learn maintainers therefore strongly discourage the use of\n",
      "this dataset unless the purpose of the code is to study and educate\n",
      "about ethical issues in data science and machine learning.\n",
      "\n",
      "In this special case, you can fetch the dataset from the original\n",
      "source::\n",
      "\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "    target = raw_df.values[1::2, 2]\n",
      "\n",
      "Alternative datasets include the California housing dataset and the\n",
      "Ames housing dataset. You can load the datasets as follows::\n",
      "\n",
      "    from sklearn.datasets import fetch_california_housing\n",
      "    housing = fetch_california_housing()\n",
      "\n",
      "for the California housing dataset and::\n",
      "\n",
      "    from sklearn.datasets import fetch_openml\n",
      "    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "for the Ames housing dataset.\n",
      "\n",
      "[1] M Carlisle.\n",
      "\"Racist data destruction?\"\n",
      "<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n",
      "\n",
      "[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n",
      "\"Hedonic housing prices and the demand for clean air.\"\n",
      "Journal of environmental economics and management 5.1 (1978): 81-102.\n",
      "<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
      ": LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n"
     ]
    }
   ],
   "source": [
    "# import relevant dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "from aif360.metrics import ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    X: pd.DataFrame,\n",
    "    y,\n",
    "    protected_attribute_name: str\n",
    ") -> StandardDataset:\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = pd.Series(y.flatten(), index=X.index, name='class')\n",
    "    return StandardDataset(\n",
    "        df=pd.concat([X, y], axis=1),\n",
    "        label_name=\"class\",\n",
    "        favorable_classes=[1],\n",
    "        protected_attribute_names=[protected_attribute_name],\n",
    "        privileged_classes=[[1]],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\I2044\\Anaconda3\\envs\\python38\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# fetch raw-data from sklearn.datasets\n",
    "raw_data = fetch_openml(data_id=1590, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_raw = pd.get_dummies(raw_data.data)\n",
    "X_raw = pd.DataFrame(MinMaxScaler().fit_transform(X_raw), columns=X_raw.columns)\n",
    "y = 1 * (raw_data.target == \">50K\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.5, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute_name = \"sex_Male\"\n",
    "\n",
    "privileged_groups = [{protected_attribute_name: 1.0}]\n",
    "unprivileged_groups = [{protected_attribute_name: 0.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\I2044\\Anaconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.648401\n",
      "epoch 1; iter: 0; batch classifier loss: 0.385139\n",
      "epoch 2; iter: 0; batch classifier loss: 0.306398\n",
      "epoch 3; iter: 0; batch classifier loss: 0.361695\n",
      "epoch 4; iter: 0; batch classifier loss: 0.286632\n",
      "epoch 5; iter: 0; batch classifier loss: 0.438883\n",
      "epoch 6; iter: 0; batch classifier loss: 0.289754\n",
      "epoch 7; iter: 0; batch classifier loss: 0.346614\n",
      "epoch 8; iter: 0; batch classifier loss: 0.327000\n",
      "epoch 9; iter: 0; batch classifier loss: 0.320092\n",
      "epoch 10; iter: 0; batch classifier loss: 0.371554\n",
      "epoch 11; iter: 0; batch classifier loss: 0.324482\n",
      "epoch 12; iter: 0; batch classifier loss: 0.319571\n",
      "epoch 13; iter: 0; batch classifier loss: 0.340679\n",
      "epoch 14; iter: 0; batch classifier loss: 0.396512\n",
      "epoch 15; iter: 0; batch classifier loss: 0.343625\n",
      "epoch 16; iter: 0; batch classifier loss: 0.332313\n",
      "epoch 17; iter: 0; batch classifier loss: 0.285954\n",
      "epoch 18; iter: 0; batch classifier loss: 0.336866\n",
      "epoch 19; iter: 0; batch classifier loss: 0.219669\n",
      "epoch 20; iter: 0; batch classifier loss: 0.347676\n",
      "epoch 21; iter: 0; batch classifier loss: 0.283266\n",
      "epoch 22; iter: 0; batch classifier loss: 0.291555\n",
      "epoch 23; iter: 0; batch classifier loss: 0.268568\n",
      "epoch 24; iter: 0; batch classifier loss: 0.255812\n",
      "epoch 25; iter: 0; batch classifier loss: 0.309093\n",
      "epoch 26; iter: 0; batch classifier loss: 0.272128\n",
      "epoch 27; iter: 0; batch classifier loss: 0.248107\n",
      "epoch 28; iter: 0; batch classifier loss: 0.383359\n",
      "epoch 29; iter: 0; batch classifier loss: 0.351657\n",
      "epoch 30; iter: 0; batch classifier loss: 0.245664\n",
      "epoch 31; iter: 0; batch classifier loss: 0.338551\n",
      "epoch 32; iter: 0; batch classifier loss: 0.290014\n",
      "epoch 33; iter: 0; batch classifier loss: 0.185624\n",
      "epoch 34; iter: 0; batch classifier loss: 0.256270\n",
      "epoch 35; iter: 0; batch classifier loss: 0.290644\n",
      "epoch 36; iter: 0; batch classifier loss: 0.233842\n",
      "epoch 37; iter: 0; batch classifier loss: 0.386208\n",
      "epoch 38; iter: 0; batch classifier loss: 0.339469\n",
      "epoch 39; iter: 0; batch classifier loss: 0.239211\n",
      "epoch 40; iter: 0; batch classifier loss: 0.298406\n",
      "epoch 41; iter: 0; batch classifier loss: 0.228380\n",
      "epoch 42; iter: 0; batch classifier loss: 0.265374\n",
      "epoch 43; iter: 0; batch classifier loss: 0.263044\n",
      "epoch 44; iter: 0; batch classifier loss: 0.332955\n",
      "epoch 45; iter: 0; batch classifier loss: 0.227064\n",
      "epoch 46; iter: 0; batch classifier loss: 0.231705\n",
      "epoch 47; iter: 0; batch classifier loss: 0.245564\n",
      "epoch 48; iter: 0; batch classifier loss: 0.306155\n",
      "epoch 49; iter: 0; batch classifier loss: 0.282458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x24e26fdfd90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "# Learn parameters with debias set to False\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "AB_PLAIN = AdversarialDebiasing(\n",
    "    privileged_groups = privileged_groups,\n",
    "    unprivileged_groups = unprivileged_groups,\n",
    "    scope_name='plain_classifier',\n",
    "    debias=False,\n",
    "    sess=sess\n",
    ")\n",
    "dataset_train = create_dataset(X_train, y_train, protected_attribute_name)\n",
    "AB_PLAIN.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((X_test.shape[0],1))\n",
    "dataset_test = create_dataset(X_test, y, protected_attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_pred_plain = AB_PLAIN.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.219000819000819"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_pred_plain.labels.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.782194; batch adversarial loss: 0.725723\n",
      "epoch 1; iter: 0; batch classifier loss: 0.421570; batch adversarial loss: 0.637983\n",
      "epoch 2; iter: 0; batch classifier loss: 0.405619; batch adversarial loss: 0.636180\n",
      "epoch 3; iter: 0; batch classifier loss: 0.394683; batch adversarial loss: 0.634207\n",
      "epoch 4; iter: 0; batch classifier loss: 0.372450; batch adversarial loss: 0.636455\n",
      "epoch 5; iter: 0; batch classifier loss: 0.374253; batch adversarial loss: 0.643077\n",
      "epoch 6; iter: 0; batch classifier loss: 0.388330; batch adversarial loss: 0.680611\n",
      "epoch 7; iter: 0; batch classifier loss: 0.347921; batch adversarial loss: 0.623439\n",
      "epoch 8; iter: 0; batch classifier loss: 0.391733; batch adversarial loss: 0.623459\n",
      "epoch 9; iter: 0; batch classifier loss: 0.325650; batch adversarial loss: 0.622758\n",
      "epoch 10; iter: 0; batch classifier loss: 0.346007; batch adversarial loss: 0.574849\n",
      "epoch 11; iter: 0; batch classifier loss: 0.285128; batch adversarial loss: 0.597595\n",
      "epoch 12; iter: 0; batch classifier loss: 0.375630; batch adversarial loss: 0.590438\n",
      "epoch 13; iter: 0; batch classifier loss: 0.374981; batch adversarial loss: 0.613724\n",
      "epoch 14; iter: 0; batch classifier loss: 0.334302; batch adversarial loss: 0.718372\n",
      "epoch 15; iter: 0; batch classifier loss: 0.153169; batch adversarial loss: 0.635023\n",
      "epoch 16; iter: 0; batch classifier loss: 0.287475; batch adversarial loss: 0.627771\n",
      "epoch 17; iter: 0; batch classifier loss: 0.341919; batch adversarial loss: 0.628100\n",
      "epoch 18; iter: 0; batch classifier loss: 0.352268; batch adversarial loss: 0.580955\n",
      "epoch 19; iter: 0; batch classifier loss: 0.426183; batch adversarial loss: 0.630334\n",
      "epoch 20; iter: 0; batch classifier loss: 0.399029; batch adversarial loss: 0.638277\n",
      "epoch 21; iter: 0; batch classifier loss: 0.369079; batch adversarial loss: 0.599621\n",
      "epoch 22; iter: 0; batch classifier loss: 0.318445; batch adversarial loss: 0.604239\n",
      "epoch 23; iter: 0; batch classifier loss: 0.319358; batch adversarial loss: 0.636797\n",
      "epoch 24; iter: 0; batch classifier loss: 0.381438; batch adversarial loss: 0.700559\n",
      "epoch 25; iter: 0; batch classifier loss: 0.315883; batch adversarial loss: 0.616680\n",
      "epoch 26; iter: 0; batch classifier loss: 0.347804; batch adversarial loss: 0.608356\n",
      "epoch 27; iter: 0; batch classifier loss: 0.277568; batch adversarial loss: 0.608931\n",
      "epoch 28; iter: 0; batch classifier loss: 0.275462; batch adversarial loss: 0.641906\n",
      "epoch 29; iter: 0; batch classifier loss: 0.320943; batch adversarial loss: 0.619217\n",
      "epoch 30; iter: 0; batch classifier loss: 0.369436; batch adversarial loss: 0.598724\n",
      "epoch 31; iter: 0; batch classifier loss: 0.288399; batch adversarial loss: 0.648657\n",
      "epoch 32; iter: 0; batch classifier loss: 0.320081; batch adversarial loss: 0.650519\n",
      "epoch 33; iter: 0; batch classifier loss: 0.262778; batch adversarial loss: 0.664727\n",
      "epoch 34; iter: 0; batch classifier loss: 0.252318; batch adversarial loss: 0.626815\n",
      "epoch 35; iter: 0; batch classifier loss: 0.242569; batch adversarial loss: 0.644010\n",
      "epoch 36; iter: 0; batch classifier loss: 0.391412; batch adversarial loss: 0.620907\n",
      "epoch 37; iter: 0; batch classifier loss: 0.256386; batch adversarial loss: 0.628863\n",
      "epoch 38; iter: 0; batch classifier loss: 0.276192; batch adversarial loss: 0.657658\n",
      "epoch 39; iter: 0; batch classifier loss: 0.341326; batch adversarial loss: 0.592139\n",
      "epoch 40; iter: 0; batch classifier loss: 0.259998; batch adversarial loss: 0.584111\n",
      "epoch 41; iter: 0; batch classifier loss: 0.373558; batch adversarial loss: 0.597154\n",
      "epoch 42; iter: 0; batch classifier loss: 0.318694; batch adversarial loss: 0.680834\n",
      "epoch 43; iter: 0; batch classifier loss: 0.254843; batch adversarial loss: 0.653519\n",
      "epoch 44; iter: 0; batch classifier loss: 0.256837; batch adversarial loss: 0.634406\n",
      "epoch 45; iter: 0; batch classifier loss: 0.280756; batch adversarial loss: 0.586535\n",
      "epoch 46; iter: 0; batch classifier loss: 0.324444; batch adversarial loss: 0.609045\n",
      "epoch 47; iter: 0; batch classifier loss: 0.264233; batch adversarial loss: 0.634411\n",
      "epoch 48; iter: 0; batch classifier loss: 0.247137; batch adversarial loss: 0.694988\n",
      "epoch 49; iter: 0; batch classifier loss: 0.315588; batch adversarial loss: 0.591090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x24e296de4c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "# Learn parameters with debias set to False\n",
    "\n",
    "AB_DEBIASED = AdversarialDebiasing(\n",
    "    privileged_groups = privileged_groups,\n",
    "    unprivileged_groups = unprivileged_groups,\n",
    "    scope_name='debiased_classifier',\n",
    "    debias=True,\n",
    "    sess=sess\n",
    ")\n",
    "AB_DEBIASED.fit(dataset_train)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_pred_debiased = AB_DEBIASED.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15331695331695333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_pred_debiased.labels.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_plain = ClassificationMetric(\n",
    "    dataset_test,\n",
    "    dataset_test_pred_plain,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "metric_debiased = ClassificationMetric(\n",
    "    dataset_test,\n",
    "    dataset_test_pred_debiased,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3117628783513548"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_plain.disparate_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9442368534700252"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_debiased.disparate_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\I2044\\Anaconda3\\envs\\python38\\lib\\site-packages\\aif360\\metrics\\classification_metric.py:278: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "c:\\Users\\I2044\\Anaconda3\\envs\\python38\\lib\\site-packages\\aif360\\metrics\\classification_metric.py:279: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.780999180999181"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_plain.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8466830466830467"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_debiased.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
